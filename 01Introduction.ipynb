{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Current News using Natural Language Processing\n",
    "\n",
    "This set of notebooks is a part of the research paper published by Dow Jones. It is an overview of NLP methods and DNA platform use cases. It is divded into nine notebooks where the first four are related to the NLP in general. The next five notebooks are use cases of NLP methods that can be used by financial companiesusing using the DNA platform. The notebooks are divided in the order:\n",
    "\n",
    "1. Introduction with historical overview of NLP\n",
    "2. Text processing methods\n",
    "3. Language understanding methods\n",
    "4. Overview of deep learning methods used in NLP\n",
    "5. Use case: Terms relationships based on Jeff Bezos news article terms relationship\n",
    "6. Use case: Brexit news intent recognition\n",
    "7. Use case: Time series sentiment analysis on blockchain and bitcoins\n",
    "8. Use case: Climate change text summarization\n",
    "9. Use case: Headline generation solution on stock indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical review\n",
    "\n",
    "The work on understanding the natural language started in ancient times. The first solutions were machine translations used for Russian to English translations. Currently, there are more and more shallow or deep neural networks applications in natural language processing. Some of such topics are described in the last section of this paper.\n",
    "\n",
    "![timeline](images/timeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work on understanding the natural language started in ancient times. Based on \\cite{Jones1994} it started around 1950. The first solutions were machine translations used for Russian to English translations. The same paper divides the history of NLP into four phases: late 1940s to late 1960s, late 1960s to late 1970s, late 1970s to late 1980s and the 1990s. The timeline above is prepared on this and other papers, and contain NLP history events from the 1950s until now. \n",
    "\n",
    "It starts with the Turing test **[1]**, because it is a benchmark of how to measure intelligence and reflects to language understanding. In 1954 two paper were published on machine translation **[2]** and bag of words **[3]**. The next important step is the grammars introduced by Chomsky in 1957 **[4]**. It allows defining the grammar and language with graphs. Make it possible to generate a text that follows a defined grammar. Another step in NLP was the first implementation of a question answering system **[5]**. Five years later, a first chatbot was implemented **[6]**. After augmented transition network introduced in 1970, the AI winter happened that affected also the research on NLP as the current solutions are mostly based on machine and deep learning methods. This is why not so many innovative research results were published in the 1970s and 1980s. The one research worth to mention is the first chatbot that generated English prose that was introduced in 1984**[7]**. \n",
    "\n",
    "In the 1990s more and more interesting papers were published like the one from 1996**[8]** when another very important paper was published. This is when named entity recognition term was introduced. The revival of natural language understanding came in the next century with word embedding in 2001**[9]** and next researches. It allowed moving from text to a vector which can be easily handled by any machine learning method. It was followed with word2vec introduced by Mikolov in 2013**[10]**. Word2Vec embeds words in a multidimensional space respecting relations between them based on the context that the word is used in. Such an approach gives to a better understanding of the meaning of each word. One of the first research paper on emotions extraction was introduced by Panny and Lee in 2002**[11]**. It is also known as sentiment analysis and give us the knowledge of emotions that are behind a sentence or whole document. In 2006 IBM Watson was used against humans in Jeopardy. The methods used were able to understand the text and reply with meaningful answers. \n",
    "\n",
    "In the XXI century, many cases were found where neural networks are applied for natural language processing. In 2008 and next followed by research in 2013 by the same Authors, neural networks are used for multi-tasking**[12]**. Multi-tasking is about many models used for different purpose or tasks, that share the parameters between each other. Usually, it gives better results. Other neural networks that were used and are used since now are different types or recurrent neural networks architectures like LSTM**[13]**. Another use case is Seq2Seq model that can be used for text translation and text generation**[14]**. Very interesting research that was recently published is on attention neural networks. Attention network is a deep learning method that is a neural machine translation architecture that has a memory within the network that allows visualizing the weights matrix**[15]**. Currently, there are more and more shallow or deep neural networks applications in natural language processing. Some of such topics are described in the last section of this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "**[1]** Turing, A. M., 1950 Computing machinery and intelligence. Mind, pp. 433—-460.\n",
    "\n",
    "**[2]** Hutchins, J., 2004 The first public demonstration of machine translation: the georgetown-ibm system, 7th january 1954.\n",
    "\n",
    "**[3]** Harris, Z., 1954 Distributional structure.Word 10: 146–162.\n",
    "\n",
    "**[4]** Chomsky, N., 1957 Syntactic Structures. Mouton.\n",
    "\n",
    "**[5]** Green, B. F., A. K.Wolf, C. Chomsky, and K. Laughery, 1961 Baseball: An automatic question-answerer. In Papers Presented at the May 9-11, 1961, Western Joint IRE-AIEE-ACM Computer Conference, IRE-AIEE-ACM ’61 (Western), pp. 219–224, ACM.\n",
    "\n",
    "**[6]** Weizenbaum, J., 1966 Eliza&mdash;a computer program for the study of natural language communication between man and machine. Communications of the ACM 9: 36–45.\n",
    "\n",
    "**[7]** Chamberlain, W. and T. Etter, 1984The Policeman’s Beard is Half-Constructed: Computer Prose and Poetry. Warner Books.\n",
    "\n",
    "**[8]** Grishman, R. and B. Sundheim, 1996 Message understanding conference-6: A brief history. In Proceedings of the 16th Conference on Computational Linguistics - Volume 1, pp. 466–471, Association for Computational Linguistics.\n",
    "\n",
    "**[9]** Bengio, Y., R. Ducharme, and P. Vincent, 2001 A neural probabilistic language model.\n",
    "\n",
    "**[10]** Mikolov, T., K. Chen, G. Corrado, and J. Dean, 2013a Efficient estimation of word representations in vector space. ICLR.\n",
    "\n",
    "**[11]** Pang, B., L. Lee, and S. Vaithyanathan, 2002 Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP, pp. 79–86.\n",
    "\n",
    "**[12]** Collobert, R. and J.Weston, 2008 A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pp. 160–167, ACM.\n",
    "\n",
    "**[13]** Graves, A., N. Jaitly, and A. rahman Mohamed, 2013 Hybrid speech recognition with deep bidirectional lstm. In In IEEE Workshop on Automatic Speech Recognition and Understanding.\n",
    "\n",
    "**[14]** Sutskever, I., O. Vinyals, and Q. V. Le, 2014 Sequence to sequence learning with neural networks. CoRR.\n",
    "\n",
    "**[15]** Bahdanau, D., K. Cho, and Y. Bengio, 2014 Neural machine translation by jointly learning to align and translate. CoRR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
