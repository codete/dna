{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing methods \n",
    "\n",
    "The most basic methods that are currently used for text processing are:\n",
    "\n",
    "- tokenization\n",
    "- pos taging,\n",
    "- lemmatization,\n",
    "- stemming,\n",
    "- noun chunks,\n",
    "- named entity recognition.\n",
    "\n",
    "In this notebook we go through the methods and show how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "The process of tokenization can be summarized as a method that splits text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dow', 'Jones', 'is', 'a', 'fintech', 'company.']\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Dow Jones is a fintech company.\"\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = \"\\\\s+\"\n",
    "words = re.split(pattern, example_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLP tools such as NLP or SpaCy, it can easily tokenize a sentence to get a list of tokens that are meaningful for researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Dow', 'Jones', 'is', 'a', 'fintech', 'company', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "print(\"Tokens: \" + str(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another task that can be solved with tokenization is sentence split. It can be again done using many NLP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codete/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dow Jones is a fintech company.', 'We analyze news with NLP.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "example_sentences = \"Dow Jones is a fintech company. We analyze news with NLP.\"\n",
    "\n",
    "sent_tokenize(example_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both examples work well for typical documents, but sometimes it may need to handle text from different sources like social media. In such a case, it needs to deal with additional symbols like emojis or hashtags. Regular tokenizer will handle each sign in an emoji or a hashtag in an improper way. A solution for that kind of text are customized tokenizers like the Tweets tokenizer that is already available within NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', '#', 'hashtag', ':', ':', '-', ')', ':', '-P', '<', '3', 'and', 'some', 'arrows', '<', '>', '-', '>', '<', '--']\n",
      "['This', 'is', 'a', '#hashtag', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->', '<--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer # Yes, this tokenizer was based on Tweets\n",
    "\n",
    "text = \"This is a #hashtag: :-) :-P <3 and some arrows < > -> <--\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "twitter_tokenizer = TweetTokenizer()\n",
    "tokens = twitter_tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tagging\n",
    "\n",
    "Part of speech tagging is a popular method that tags each token with part of speech. A PoS tag is a short name like JJ or NN that gives the part of speech, but also more details like grammatical category or mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged: [('Dow', 'NNP'), ('Jones', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('fintech', 'JJ'), ('company', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Dow Jones is a fintech company.\"\n",
    "tokens = nltk.word_tokenize(example_text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(\"Tagged: \" + str(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging is a popular method that tags each token with part of speech. A PoS tag is a short name like JJ or NN that gives the part of speech, but also more details like grammatical category or mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  \n",
      "\n",
      " _SP SPACE\n",
      "1 Pierre NNP PROPN\n",
      "2 Vinken NNP PROPN\n",
      "3 , , PUNCT\n",
      "4 61 CD NUM\n",
      "5 years NNS NOUN\n",
      "6 old JJ ADJ\n",
      "7 , , PUNCT\n",
      "8 will MD VERB\n",
      "9 join VB VERB\n",
      "10 the DT DET\n",
      "11 board NN NOUN\n",
      "12 as IN ADP\n",
      "13 a DT DET\n",
      "14 nonexecutive JJ ADJ\n",
      "15 director NN NOUN\n",
      "16 Nov. NNP PROPN\n",
      "17 29 CD NUM\n",
      "18 . . PUNCT\n",
      "19 \n",
      "  SPACE\n",
      "20 Mr. NNP PROPN\n",
      "21 Vi NNP PROPN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "treebank.fileids()\n",
    "raw = nltk.corpus.treebank_raw.raw()[0:100].replace('.START','').rstrip(\"\\r\\n\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(raw)\n",
    "\n",
    "for span in doc.sents:\n",
    "    for i in range(span.start, span.end):\n",
    "        token = doc[i]\n",
    "        print(i, token.text, token.tag_, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Each word can be written differently depending on the inflection form used. There are many possibilities, but based on a specific part of speech like a noun or adjective, it has a root word from which the word changes. Lemmatization is the process of getting the root word based on a specific part of speech\n",
    "\n",
    "![timeline](images/lemmatization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(wordnet_lemmatizer.lemmatize('do',pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('does',pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize('doing',pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base depends also on the part of speech that it should transform into. In NLTK it can be changed by setting the **pos** parameter to one of the values ``a``, ``s``, ``r``, ``n``, ``v``. Each value stands for a different part of speech of the base word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('are',pos='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "The goal of stemming is also to reduce a given word to a root word. The difference between lemmatization and stemming is that stemming uses a set of rules how the word is reduced instead of vocabulary. It means that the reduced word after stemming does not need to exist in a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'is', 'a', 'research', 'paper', 'on', 'natur', 'languag', 'process']\n",
      "['thi', 'is', 'a', 'research', 'pap', 'on', 'nat', 'langu', 'process']\n",
      "['this', 'is', 'a', 'research', 'paper', 'on', 'natur', 'languag', 'process']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sample = \"This is a research paper on natural language processing\"\n",
    "\n",
    "tokens = word_tokenize(sample)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "p_stem = [porter.stem(t) for t in tokens]\n",
    "print(p_stem)\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "l_stem = [lancaster.stem(t) for t in tokens]\n",
    "print(l_stem)\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "s_stem = [snowball.stem(t) for t in tokens]\n",
    "print(s_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chunks\n",
    "\n",
    "Noun chunks is a method to get nouns out of a text. It is much easier to understand a text by getting just a list of nouns. It can be used to retrieve information from a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a Dow Jones research paper\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "for np in doc.noun_chunks:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "A handy method that gives an even better understanding than noun chunks is the named entity recognition method. As almost all text processing methods, NER is trained for each language. Such training uses annotated datasets and machine learning methods.\n",
    "\n",
    "![timeline](images/noun_chunks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG Dow Jones\n",
      "a Dow Jones research paper\n",
      "natural language processing\n"
     ]
    }
   ],
   "source": [
    "sample = \"This is a Dow Jones research paper on natural language processing\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sample)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, entity.text)\n",
    "    \n",
    "for np in doc.noun_chunks:\n",
    "    print(np)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
