{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "\n",
    "In 2006 first deep learning method was proposed in deeplearning. Since that time more and more deep learning methods were introduced and many of such were successfully applied in many areas like object recognition on images, speech recognition, and others. Several methods can be also applied for Natural Language Processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "One of the advantages of RNN is that it can easily change size of the input and output vector. It is one of the reasons why it has many applications in NLP. Two similar networks are based on Recurrent Neural Network (RNN) -- LSTM \\cite{lstm} and Gated Recurrent Unit.\n",
    "\n",
    "![timeline](images/lstm.png)\n",
    "\n",
    "LSTMs have an additional feature compared to RNN, that it has memory cells that allow to remember dependencies between previously trained objects and solve the gradient vanishing or exploding problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "The idea of autoencoders is quite old autoencoders, but since 2008 several deep architectures of autoencoders were introduced such as: Contractive Autoencoders (CAE), Variational Autoencoders (VAE), Sparse Autoencoders (SAE), Denoising Autoencoders (DAE) and many other types and extensions of autoencoders architecture.\n",
    "\n",
    "![timeline](images/autoencoders.png)\n",
    "\n",
    "There are many applications of autoencoders for NLP. The four most important are: \n",
    "\n",
    "- text generation,\n",
    "- language translation,\n",
    "- sentiment analysis,\n",
    "- dimensionality reduction.\n",
    "\n",
    "It is an unsupervised network that trains how to generate an output based on some input. It can be thought as a kind of a compressing method. Autoencoders train how to represent the output by reducing the number of features that represent it. The specific variations of autoencoders realize different goals. In CAE the focus is on derivatives and having it small, what is achieved by adding a penalty term. It performs better than other types of autoencoders. The main goal of DAE is focused on input correction and accurate reconstruction of the input by some degree. VAE allows generating similar results to data that the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Network\n",
    "\n",
    "Similar to autoencoders, GAN architecture is commonly used for text or image generation. GAN consists of two parts: generator and discriminator. The generator prepares artificial data that is similar to the real-world data. The discriminator gets two types of data: artificial and real-world data. The output of the discriminator is a label. The discriminator is a convolution network, where the generator part can be used as a separate architecture for classification. There were some publications in which researchers suggested that applying GANs for NLP is very hard. The recent papers show that it is possible. Apart from text generation, GANs are used in NLP for machine translation.\n",
    "\n",
    "![timeline](images/gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Network\n",
    "\n",
    "Convolution Neural Networks (CNN) became very popular because of object recognition on images. CNN consists of many layers of different types. Each layer changes the shape of the matrix from the previous layer. The cells (black boxes) are next represented differently in the new matrices. The main idea of CNNs is to find some patterns and represent it finally with an output vector. In NLP CNNs are used for text summarization and understanding \n",
    "\n",
    "![timeline](images/cnn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
